{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import os\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import svgwrite\n",
    "from IPython.display import SVG, display\n",
    "from svg.path import Path, Line, Arc, CubicBezier, QuadraticBezier, parse_path\n",
    "from svg.path import Path, Line, Arc, CubicBezier, QuadraticBezier\n",
    "from svgpathtools import svg2paths, wsvg\n",
    "np.set_printoptions(linewidth=np.nan, precision=3)\n",
    "import cv2\n",
    "import potrace\n",
    "import matplotlib.pyplot as plt\n",
    "from rdp import rdp\n",
    "from thinning import guo_hall_thinning\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "import math\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from six.moves import xrange\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# libraries required for visualisation:\n",
    "import svgwrite # conda install -c omnia svgwrite=1.1.6\n",
    "import PIL\n",
    "from PIL import Image\n",
    "# set numpy output to something sensible\n",
    "np.set_printoptions(precision=8, edgeitems=6, linewidth=200, suppress=True)\n",
    "from sketch_rnn_class import *\n",
    "from model_class import *\n",
    "from utils_class import *\n",
    "from rnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HELPER FUNCTIONS\n",
    "def draw_strokes(data, factor=0.2, svg_filename = '/tmp/sketch_rnn/svg/sample.svg'):\n",
    "  tf.gfile.MakeDirs(os.path.dirname(svg_filename))\n",
    "  min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "  dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "  dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "  dwg.add(dwg.rect(insert=(0, 0), size=dims,fill='white'))\n",
    "  lift_pen = 1\n",
    "  abs_x = 25 - min_x \n",
    "  abs_y = 25 - min_y\n",
    "  p = \"M%s,%s \" % (abs_x, abs_y)\n",
    "  command = \"m\"\n",
    "  for i in xrange(len(data)):\n",
    "    if (lift_pen == 1):\n",
    "      command = \"m\"\n",
    "    elif (command != \"l\"):\n",
    "      command = \"l\"\n",
    "    else:\n",
    "      command = \"\"\n",
    "    x = float(data[i,0])/factor\n",
    "    y = float(data[i,1])/factor\n",
    "    lift_pen = data[i, 2]\n",
    "    p += command+str(x)+\",\"+str(y)+\" \"\n",
    "  the_color = \"black\"\n",
    "  stroke_width = 1\n",
    "  dwg.add(dwg.path(p).stroke(the_color,stroke_width).fill(\"none\"))\n",
    "  dwg.save()\n",
    "  display(SVG(dwg.tostring()))\n",
    "\n",
    "def get_bounds(data, factor=10):\n",
    "  \"\"\"Return bounds of data.\"\"\"\n",
    "  min_x = 0\n",
    "  max_x = 0\n",
    "  min_y = 0\n",
    "  max_y = 0\n",
    "\n",
    "  abs_x = 0\n",
    "  abs_y = 0\n",
    "  for i in range(len(data)):\n",
    "    x = float(data[i, 0]) / factor\n",
    "    y = float(data[i, 1]) / factor\n",
    "    abs_x += x\n",
    "    abs_y += y\n",
    "    min_x = min(min_x, abs_x)\n",
    "    min_y = min(min_y, abs_y)\n",
    "    max_x = max(max_x, abs_x)\n",
    "    max_y = max(max_y, abs_y)\n",
    "\n",
    "  return (min_x, max_x, min_y, max_y)\n",
    "\n",
    "def cubicbezier(x0, y0, x1, y1, x2, y2, x3, y3, n=20):\n",
    "    # from http://rosettacode.org/wiki/Bitmap/B%C3%A9zier_curves/Cubic\n",
    "    pts = []\n",
    "    for i in range(n+1):\n",
    "        t = float(i) / float(n)\n",
    "        a = (1. - t)**3\n",
    "        b = 3. * t * (1. - t)**2\n",
    "        c = 3.0 * t**2 * (1.0 - t)\n",
    "        d = t**3\n",
    "\n",
    "        x = float(a * x0 + b * x1 + c * x2 + d * x3)\n",
    "        y = float(a * y0 + b * y1 + c * y2 + d * y3)\n",
    "        pts.append( (x, y) )\n",
    "    return pts\n",
    "\n",
    "def get_path_strings(svgfile):\n",
    "    tree = ET.parse(svgfile)\n",
    "    p = []\n",
    "    for elem in tree.iter():\n",
    "        if elem.attrib.has_key('d'):\n",
    "            p.append(elem.attrib['d'])\n",
    "    return p\n",
    "\n",
    "def build_lines(svgfile, line_length_threshold = 10.0, min_points_per_path = 1, max_points_per_path = 3):\n",
    "  # we don't draw lines less than line_length_threshold\n",
    "  path_strings = get_path_strings(svgfile)\n",
    "  lines = []\n",
    "\n",
    "  for path_string in path_strings:\n",
    "    full_path = parse_path(path_string)\n",
    "    for i in range(len(full_path)):\n",
    "      p = full_path[i]\n",
    "      if type(p) != Line and type(p) != CubicBezier:\n",
    "        print \"encountered an element that is not just a line or bezier \"\n",
    "        print \"type: \",type(p)\n",
    "        print p\n",
    "      else:\n",
    "        x_start = p.start.real\n",
    "        y_start = p.start.imag\n",
    "        x_end = p.end.real\n",
    "        y_end = p.end.imag\n",
    "        line_length = np.sqrt((x_end-x_start)*(x_end-x_start)+(y_end-y_start)*(y_end-y_start))\n",
    "        len_data.append(line_length)\n",
    "        points = []\n",
    "        if type(p) == CubicBezier:\n",
    "          x_con1 = p.control1.real\n",
    "          y_con1 = p.control1.imag\n",
    "          x_con2 = p.control2.real\n",
    "          y_con2 = p.control2.imag\n",
    "          n_points = int(line_length / line_length_threshold)+1\n",
    "          n_points = max(n_points, min_points_per_path)\n",
    "          n_points = min(n_points, max_points_per_path)\n",
    "          points = cubicbezier(x_start, y_start, x_con1, y_con1, x_con2, y_con2, x_end, y_end, n_points)\n",
    "        else:\n",
    "          points = [(x_start, y_start), (x_end, y_end)]\n",
    "        if i == 0: # only append the starting point for svg\n",
    "          lines.append([points[0][0], points[0][1], 0, 0]) # put eoc to be zero\n",
    "        for j in range(1, len(points)):\n",
    "          eos = 0\n",
    "          if j == len(points)-1 and i == len(full_path)-1:\n",
    "            eos = 1\n",
    "          lines.append([points[j][0], points[j][1], eos, 0]) # put eoc to be zero\n",
    "  lines = np.array(lines, dtype=np.float32)\n",
    "      # make it relative moves\n",
    "  lines[1:,0:2] -= lines[0:-1,0:2]\n",
    "  lines[-1,3] = 1 # end of character\n",
    "  lines[0] = [0, 0, 0, 0] # start at origin\n",
    "  return lines[1:]\n",
    "\n",
    "def lines_to_strokes(lines):\n",
    "    eos = 0\n",
    "    strokes = [[0, 0, 0]]\n",
    "    for line in lines:\n",
    "        print line\n",
    "        linelen = len(line)\n",
    "        for i in range(linelen):\n",
    "            eos = 0 if i < linelen - 1 else 1\n",
    "            strokes.append([line[i][0], line[i][1], eos])\n",
    "    strokes = np.array(strokes)\n",
    "    strokes[1:, 0:2] -= strokes[:-1, 0:2]\n",
    "    return strokes[1:, :]\n",
    "\n",
    "def encode(input_strokes):\n",
    "  strokes = to_big_strokes(input_strokes,max_len=125).tolist()\n",
    "  print(np.shape(strokes))\n",
    "  strokes.insert(0, [0, 0, 1, 0, 0])\n",
    "  seq_len = [len(input_strokes)]\n",
    "  draw_strokes(to_normal_strokes(np.array(strokes)))\n",
    "  return sess.run(eval_model.batch_z, feed_dict={eval_model.input_data: [strokes], eval_model.sequence_lengths: seq_len})[0]\n",
    "\n",
    "def preprocess_single(stroke):\n",
    "    scale_factor=0.15 #see model params\n",
    "    stroke[:, 0:2] /= scale_factor\n",
    "    return stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT FROM dx to POLYLINE format\n",
    "all_lines = []\n",
    "current_line = []\n",
    "last_x = test_lines[0][0]\n",
    "last_y = test_lines[0][1]\n",
    "test_lines = test_lines[1:]\n",
    "for row in test_lines:\n",
    "    dx = row[0]\n",
    "    dy = row[1]\n",
    "    point = [last_x+dx,last_y+dy]\n",
    "    if not(dx == 0 and dy == 0):\n",
    "        current_line.append(point)\n",
    "    if row[2] == 1 or row[3] == 1:\n",
    "        print(np.shape(current_line))\n",
    "        all_lines.append(current_line)\n",
    "        current_line = []\n",
    "    last_x = x\n",
    "    last_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT FROM X-Y TO SIMPLE POLYLINE\n",
    "all_lines = []\n",
    "current_line = []\n",
    "for row in test_lines:\n",
    "    x = row[0]\n",
    "    y = row[1]\n",
    "    point = [x,y]\n",
    "    if not(x == 0 and y == 0):\n",
    "        current_line.append(point)\n",
    "    if row[2] == 1 or row[3] == 1:\n",
    "        print(np.shape(current_line))\n",
    "        all_lines.append(current_line)\n",
    "        current_line = []\n",
    "print np.shape(all_lines)\n",
    "print np.shape(all_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERFORM RDP ON LINES\n",
    "rdp_lines = []\n",
    "for line in all_lines:\n",
    "    print np.shape(line)\n",
    "    new_line = rdp(line, epsilon = 0.2)\n",
    "    print np.shape(new_line)\n",
    "    rdp_lines.append(new_line)\n",
    "print np.shape(rdp_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD CHECKPOINT\n",
    "#load pathways\n",
    "data_dir = 'datasets/'\n",
    "models_root_dir = '' #same directory we're in now\n",
    "model_dir = 'four'\n",
    "[train_set, valid_set, test_set, hps_model, eval_hps_model, sample_hps_model] = load_env(data_dir, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the sketch-rnn model here:\n",
    "reset_graph()\n",
    "model = Model(hps_model)\n",
    "eval_model = Model(eval_hps_model, reuse=True)\n",
    "sample_model = Model(sample_hps_model, reuse=True)\n",
    "#run session\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# loads the weights from checkpoint into our model\n",
    "load_checkpoint(sess, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ IN SINGLE SAMPLE AND PREDICT\n",
    "data = np.load(\"datasets/nose.npz\")\n",
    "test_data = data['test']\n",
    "stroke = np.random.choice(test_data)\n",
    "stroke = np.array(stroke, dtype=np.float32)\n",
    "#normalize stroke by dividing by normalizing scale factor\n",
    "scale_factor = 45.8033\n",
    "stroke[:, 0:2] /= scale_factor\n",
    "draw_strokes(stroke)\n",
    "prediction = predict_model(sess, eval_model, stroke, 125)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT SINGLE SAMPLE & PREDICT\n",
    "stroke = new_test_lines\n",
    "stroke = np.array(stroke, dtype=np.float32)\n",
    "#normalize stroke by dividing by normalizing scale factor\n",
    "scale_factor = 10\n",
    "stroke[:, 0:2] /= scale_factor\n",
    "draw_strokes(stroke)\n",
    "prediction = predict_model(sess, eval_model, stroke, 125)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ENTIRE BATCH - CONVERSION to CLASSIFICATION\n",
    "data = np.load(\"datasets/nose.npz\")\n",
    "test_data = data['test']\n",
    "preds = [0,0,0,0]\n",
    "for i in range(0,100):\n",
    "    stroke = np.random.choice(test_data)\n",
    "    stroke = np.array(stroke, dtype=np.float32)\n",
    "    #normalize stroke by dividing by normalizing scale factor\n",
    "    scale_factor = 45.8033\n",
    "    stroke[:, 0:2] /= scale_factor\n",
    "    draw_strokes(stroke,svg_filename=\"features_to_stroke/test.svg\")\n",
    "    #read in SVG\n",
    "    test_lines = build_lines(\"features_to_stroke/test.svg\")\n",
    "    #print test_lines\n",
    "    new_test_lines = test_lines[:,:3]\n",
    "    new_test_lines[1:, 0:2] -= new_test_lines[:-1, 0:2]\n",
    "    #draw_strokes(new_test_lines,factor=0.2)\n",
    "    stroke = new_test_lines\n",
    "    stroke = np.array(stroke, dtype=np.float32)\n",
    "    #normalize stroke by dividing by normalizing scale factor\n",
    "    scale_factor = 10\n",
    "    stroke[:, 0:2] /= scale_factor\n",
    "    #draw_strokes(stroke,factor=0.02)\n",
    "    prediction = int(predict_model(sess, eval_model, stroke, 125))\n",
    "    preds[prediction] += 1\n",
    "print(preds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
